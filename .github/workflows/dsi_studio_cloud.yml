name: DSI Studio Cloud
on: 
  workflow_dispatch:
    inputs:
      dataset_id:
        description: 'OpenNeuro Accession Number'
        default: ds001378
        required: true
      
jobs:    
  info:
    name: Download OpenNeuro
    runs-on: ubuntu-20.04
    outputs:
      subjects: ${{ steps.data.outputs.subjects }}
      files: ${{ steps.data.outputs.files }}
      dataset_name: ${{ steps.data.outputs.dataset_name }}
      metrics: ${{ steps.data.outputs.metrics }}
    steps:
      - name: OpenNeuro
        id: data
        run: |
          # Process dataset_description.json
          aws s3 sync --no-sign-request --region eu-west-1 --exclude "*" --include "dataset_description.json" s3://openneuro.org/${{ inputs.dataset_id }} .
          content=$(cat dataset_description.json)
          name=$(echo "$content" | jq -r '.Name')
          echo "dataset_name=$name" >> $GITHUB_OUTPUT

          # Get subjects
          SUBJECTS=$(aws s3 ls --no-sign-request --region eu-west-1 s3://openneuro.org/${{ inputs.dataset_id }} --recursive | grep '/dwi/' | awk '{print $NF}' | awk -F'/dwi/' '{print $1}' | awk -F'/sub-' '/sub-/ {print "sub-" $2}' | sort -u | jq -R -s -c 'split("\n")[:-1]')
          FILES=$(echo "$SUBJECTS" | tr '/' '_')
          echo "subjects=$SUBJECTS" >> $GITHUB_OUTPUT
          echo "files=$FILES" >> $GITHUB_OUTPUT

          metrics="${{ github.event.inputs.metrics }}"
          metrics=$(echo "[\"${metrics//,/\",\"}\"]")
          echo "metrics=$metrics" >> $GITHUB_OUTPUT

          echo "OpenNeuro ${{ inputs.dataset_id }}" >> $GITHUB_STEP_SUMMARY
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Create Release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if gh release view ${{ inputs.dataset_id }} &> /dev/null; then
            echo "Release ${{ inputs.dataset_id }} already exists, skipping release creation."
          else
            gh release create ${{ inputs.dataset_id }} --title ${{ steps.data.outputs.dataset_name }} --notes "Processed data for dataset ${{ inputs.dataset_id }}"
          fi
          
  download_data:
    uses: ./.github/workflows/nii2src.yml
    needs: info
    name: Call NII to SRC
    with:
      dataset_id: ${{ inputs.dataset_id }} 
      
  src_quality_check1:
    uses: ./.github/workflows/src_qc.yml
    needs: download_data
    name: Call NII to SRC
    with:
      src_list: 'raw.src.gz'
      dataset_id: ${{ inputs.dataset_id }}  
          
  topup_eddy:
    uses: ./.github/workflows/topup_eddy.yml
    needs: download_data
    name: Preprocess SRC
    with:
      dataset_id: ${{ inputs.dataset_id }}             
          
  src_quality_check2:
    uses: ./.github/workflows/src_qc.yml
    needs: topup_eddy
    name: SRC Quality Check
    with:
      src_list: 'src.gz'
      dataset_id: ${{ inputs.dataset_id }} 

  reconstruction:
    uses: ./.github/workflows/recon.yml
    needs: topup_eddy
    name: SRC to FIB
    with:
      dataset_id: ${{ inputs.dataset_id }}
          
  database_construction:
    uses: ./.github/workflows/create_db.yml
    needs: reconstruction
    strategy:
      matrix:
          metrics: ['qa','dti_fa']
    name: Create Database
    with:
      metrics: ${{ matrix.metrics}}
      dataset_id: ${{ inputs.dataset_id }}
